% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/config.R
\name{gpt_bigcode_config}
\alias{gpt_bigcode_config}
\alias{gpt_bigcode_config_from_pretrained}
\title{Creates a configuration dict for GPTBigCode}
\usage{
gpt_bigcode_config(
  vocab_size = 50257,
  n_positions = 1024,
  n_embd = 768,
  n_layer = 12,
  n_head = 12,
  n_inner = NULL,
  activation_function = "gelu_pytorch_tanh",
  resid_pdrop = 0.1,
  embd_pdrop = 0.1,
  attn_pdrop = 0.1,
  layer_norm_epsilon = 1e-05,
  initializer_range = 0.02,
  scale_attn_weights = TRUE,
  use_cache = TRUE,
  bos_token_id = 50256,
  eos_token_id = 50256,
  attention_softmax_in_fp32 = TRUE,
  scale_attention_softmax_in_fp32 = TRUE,
  multi_query = TRUE,
  ...
)

gpt_bigcode_config_from_pretrained(identifier, revision = "main")
}
\arguments{
\item{vocab_size}{(int, optional, defaults to 50432) — Vocabulary size of the GPTNeoX model. Defines the number of different tokens that can be represented by the inputs_ids passed when calling GPTNeoXModel.}

\item{initializer_range}{(float, optional, defaults to 1e-5) — The standard deviation of the truncated_normal_initializer for initializing all weight matrices.}

\item{use_cache}{(bool, optional, defaults to True) — Whether or not the model should return the last key/values attentions (not used by all models). Only relevant if config.is_decoder=True.}

\item{...}{Additional configuration options.}

\item{identifier}{The HuggingFace repository to download the model from.}

\item{revision}{The repository revision to download from. Either a branch name
or a commit hash.}
}
\description{
Creates a configuration dict for GPTBigCode
}
\section{Functions}{
\itemize{
\item \code{gpt_bigcode_config_from_pretrained()}: Load a config from a modelhub model

}}
